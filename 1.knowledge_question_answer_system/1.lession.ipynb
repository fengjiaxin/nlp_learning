{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 编辑距离的计算\n",
    "编辑距离可以用来计算两个字符串的相似度，它的应用场景很多，其中之一是拼写纠正（spell correction）。 编辑距离的定义是给定两个字符串str1和str2, 我们要计算通过最少多少代价cost可以把str1转换成str2. \n",
    "\n",
    "举个例子：\n",
    "\n",
    "输入:   str1 = \"geek\", str2 = \"gesek\"\n",
    "输出:  1\n",
    "插入 's'即可以把str1转换成str2\n",
    "\n",
    "输入:   str1 = \"cat\", str2 = \"cut\"\n",
    "输出:  1\n",
    "用u去替换a即可以得到str2\n",
    "\n",
    "输入:   str1 = \"sunday\", str2 = \"saturday\"\n",
    "输出:  3\n",
    "\n",
    "我们假定有三个不同的操作： 1. 插入新的字符   2. 替换字符   3. 删除一个字符。 每一个操作的代价为1. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 基于动态规划的方法\n",
    "def get_mindistance(word1,word2):\n",
    "    m = len(word1)\n",
    "    n = len(word2)\n",
    "    dp = [[0 for i in range(n+1)] for i in range(m+1)]\n",
    "    \n",
    "    # 初始化空字符串\n",
    "    for i in range(1,m+1):\n",
    "        dp[i][0] = i\n",
    "    \n",
    "    for i in range(1,n+1):\n",
    "        dp[0][i] = i\n",
    "        \n",
    "    # 动态规划\n",
    "    for i in range(1,m+1):\n",
    "        for j in range(1,n+1):\n",
    "            # 增加操作，str1a变为str2，然后再加上b，得到str2b\n",
    "            insert_op = dp[i][j-1] + 1\n",
    "            # 删除操作 ，str1a删除a后，再由str1变为str2b\n",
    "            delete_op = dp[i-1][j] + 1\n",
    "            # 替换操作 str1替换成str2\n",
    "            replace_op = dp[i-1][j-1]\n",
    "            # 添加判断a和b是否相等\n",
    "            if word1[i-1] != word2[j-1]:\n",
    "                replace_op += 1\n",
    "            dp[i][j] = min(insert_op,delete_op,replace_op)\n",
    "    return dp[m][n]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n"
     ]
    }
   ],
   "source": [
    "min_distance = get_mindistance('zhanghua','zhanghau')\n",
    "print(min_distance)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 生成指定编辑距离的单词\n",
    "给定一个单词，我们也可以生成编辑距离为K的单词列表。 比如给定 str=\"apple\"，K=1, 可以生成“appl”, \"appla\", \"pple\"...等\n",
    "下面看怎么生成这些单词。 还是用英文的例子来说明。 仍然假设有三种操作 - 插入，删除，替换"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "281\n"
     ]
    }
   ],
   "source": [
    "# 老师给的例子\n",
    "def generate_edit_one(str):\n",
    "    \"\"\"\n",
    "    给定一个字符串，生成编辑距离为1的字符串列表。\n",
    "    \"\"\"\n",
    "    letters    = 'abcdefghijklmnopqrstuvwxyz'\n",
    "    splits = [(str[:i], str[i:])for i in range(len(str)+1)]\n",
    "    inserts = [L + c + R for L, R in splits for c in letters]\n",
    "    deletes = [L + R[1:] for L, R in splits if R]\n",
    "    replaces = [L + c + R[1:] for L, R in splits if R for c in letters]\n",
    "    \n",
    "    #return set(splits)\n",
    "    return list(set(inserts + deletes + replaces))\n",
    "\n",
    "print (len(generate_edit_one(\"apple\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['sapple', 'apkple', 'aptple', 'apuple', 'applo']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generate_edit_one(\"apple\")[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 自己写的生成编辑距离为1的单词候选集合\n",
    "def get_edit_one_set(word):\n",
    "    '''\n",
    "    首先英文字母一共只有26个\n",
    "    编辑距离无非是由三种操}作\n",
    "    首先需要确定能够操作的位置\n",
    "    首先假设word = app\n",
    "    那么可以替换的位置有4个\n",
    "    可以将word进行切割，切割后的集合为{('','app'),('a','pp'),('ap','p'),('app','')}\n",
    "    1.insert操作，insert的操作可以在四个位置，例如添加c,'' + c + 'app',...\n",
    "    2.delete操作，delete的操作可以在三个位置，最后一个split无法删除\n",
    "    3.update操作，update的操作可以在三个位置，最后一个split无法替换\n",
    "    '''\n",
    "    letters = 'abcdefghijklmnopqrstuvwxyz'\n",
    "    splits = [(word[:i],word[i:]) for i in range(len(word) + 1)]\n",
    "    \n",
    "    insert_list = []\n",
    "    delete_list = []\n",
    "    update_list = []\n",
    "    \n",
    "    # 1.insert\n",
    "    for (l,r) in splits:\n",
    "        for c in letters:\n",
    "            insert_list.append(l + c + r)\n",
    "            \n",
    "    # 2.delete\n",
    "    for (l,r) in splits:\n",
    "        if r:\n",
    "            delete_list.append(l + r[1:])\n",
    "    \n",
    "    # 3.update\n",
    "    for (l,r) in splits:\n",
    "        if r:\n",
    "            for c in letters:\n",
    "                update_list.append(l + c + r[1:])\n",
    "                \n",
    "    return set(insert_list + delete_list + update_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "281\n"
     ]
    }
   ],
   "source": [
    "one_edit_set = get_edit_one_set('apple')\n",
    "print(len(one_edit_set))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "86524\n"
     ]
    }
   ],
   "source": [
    "# 老师给的例子,没考虑重复问题\n",
    "def generate_edit_two(str):\n",
    "    \"\"\"\n",
    "    给定一个字符串，生成编辑距离不大于2的字符串\n",
    "    \"\"\"\n",
    "    return [e2 for e1 in generate_edit_one(str) for e2 in generate_edit_one(e1)]\n",
    "\n",
    "print (len(generate_edit_two(\"apple\")))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 自己写的生成编辑距离为2的字符串\n",
    "def get_edit_two_set(word):\n",
    "    '''\n",
    "    先生成一个的，然后在候选集合继续生成编辑距离为1的集合\n",
    "    '''\n",
    "    result_list = []\n",
    "    one_edit_set = get_edit_one_set(word)\n",
    "    result_list.extend(list(one_edit_set))\n",
    "    for w_o in one_edit_set:\n",
    "        two_edit_set = get_edit_one_set(w_o)\n",
    "        result_list.extend(list(two_edit_set))\n",
    "        \n",
    "    return set(result_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "35334\n"
     ]
    }
   ],
   "source": [
    "two_edit_set = get_edit_two_set('apple')\n",
    "print(len(two_edit_set))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 基于结巴（jieba）的分词。 Jieba是最常用的中文分词工具~ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building prefix dict from the default dictionary ...\n",
      "Loading model from cache /var/folders/l7/rbtg9dt10vd1vkf18t5zkpd00000gn/T/jieba.cache\n",
      "Loading model cost 0.647 seconds.\n",
      "Prefix dict has been built succesfully.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Default Mode: 贪心/ 学院/ 专注/ 于/ 人工智能/ 教育\n",
      "Default Mode: 贪心学院/ 专注/ 于/ 人工智能/ 教育\n"
     ]
    }
   ],
   "source": [
    "# encoding=utf-8\n",
    "import jieba\n",
    "\n",
    "# 基于jieba的分词\n",
    "seg_list = jieba.cut(\"贪心学院专注于人工智能教育\", cut_all=False)\n",
    "print(\"Default Mode: \" + \"/ \".join(seg_list))  \n",
    "\n",
    "jieba.add_word(\"贪心学院\")\n",
    "seg_list = jieba.cut(\"贪心学院专注于人工智能教育\", cut_all=False)\n",
    "print(\"Default Mode: \" + \"/ \".join(seg_list)) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 判断一句话是否能够切分（被字典）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 老师给的例子\n",
    "dic = set([\"贪心科技\", \"人工智能\", \"教育\", \"在线\", \"专注于\"])\n",
    "def word_break(str):\n",
    "    could_break = [False] * (len(str) + 1)\n",
    "\n",
    "    could_break[0] = True\n",
    "\n",
    "    for i in range(1, len(could_break)):\n",
    "        for j in range(0, i):\n",
    "            if str[j:i] in dic and could_break[j] == True:\n",
    "                could_break[i] = True\n",
    "\n",
    "    return could_break[len(str)] == True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "assert word_break(\"贪心科技在线教育\")==True\n",
    "assert word_break(\"在线教育是\")==False\n",
    "assert word_break(\"\")==True\n",
    "assert word_break(\"在线教育人工智能\")==True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 思考题：给定一个词典和一个字符串，能不能返回所有有效的分割？ （valid segmentation) \n",
    "比如给定词典：dic = set([\"贪心科技\", \"人工智能\", \"教育\", \"在线\", \"专注于\"， “贪心”])\n",
    "和一个字符串 = “贪心科技专注于人工智能”\n",
    "\n",
    "输出为： \n",
    "“贪心” “科技” “专注于” “人工智能”\n",
    "\"贪心科技\" “专注于” “人工智能”"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['贪心科技', '专注于', '人工智能'], ['贪心', '科技', '专注于', '人工智能']]\n"
     ]
    }
   ],
   "source": [
    "def all_possible_segmentations(str,vocab):\n",
    "    '''\n",
    "    思路：返回所有的可能集合\n",
    "    首先对每个位置都建立集合list(list),外面的list代表有多少个分词方法，里面的list是具体的分词结构\n",
    "    对于 贪心科技，该result 是[['贪心','科技'],['贪心科技']]\n",
    "    '''\n",
    "    \n",
    "    '''\n",
    "    debug之后才知道几个问题\n",
    "    1.列表一定要复制\n",
    "    2.list.copy()返回一个新列表\n",
    "    '''\n",
    "    \n",
    "    # 子字符串的长度从1到len(str)\n",
    "    # 动态维护\n",
    "    break_list = [[] for i in range(len(str) + 1)]\n",
    "    break_list[0].append([''])\n",
    "\n",
    "    # 子字符串的长度从1到len(str)\n",
    "    for i in range(1, len(str) + 1):\n",
    "        # 子词开始的位置j,结束的位置i\n",
    "        for j in range(0, i):\n",
    "            if str[j:i] in vocab and len(break_list[j]) != 0:\n",
    "                #print('j:%d  i:%d' % (j, i), break_list[j])\n",
    "                for temp in break_list[j]:\n",
    "                    a = temp.copy()\n",
    "                    a.append(str[j:i])\n",
    "                    break_list[i].append(a)\n",
    "    \n",
    "    result_list = []\n",
    "    for temp in break_list[-1]:\n",
    "        result_list.append(temp[1:])\n",
    "    return result_list\n",
    " \n",
    "\n",
    "vocab = set([\"贪心科技\", \"人工智能\", \"教育\", \"在线\", \"专注于\",'贪心','科技']) \n",
    "word = \"贪心科技专注于人工智能\"\n",
    "print(all_possible_segmentations(word,vocab))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 停用词过滤\n",
    "出现频率特别高的和频率特别低的词对于文本分析帮助不大，一般在预处理阶段会过滤掉。 \n",
    "在英文里，经典的停用词为 “The”, \"an\".... "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['we', 'are', 'students']\n"
     ]
    }
   ],
   "source": [
    "# 方法1： 自己建立一个停用词词典\n",
    "stop_words = [\"the\", \"an\", \"is\", \"there\"]\n",
    "# 在使用时： 假设 word_list包含了文本里的单词\n",
    "word_list = [\"we\", \"are\", \"the\", \"students\"]\n",
    "filtered_words = [word for word in word_list if word not in stop_words]\n",
    "print (filtered_words)\n",
    "\n",
    "# 方法2：直接利用别人已经构建好的停用词库\n",
    "#from nltk.corpus import stopwords\n",
    "#cachedStopWords = stopwords.words(\"english\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "caress fli die mule deni die agre own humbl size meet state siez item sensat tradit refer colon plot\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem.porter import PorterStemmer\n",
    "stemmer = PorterStemmer()\n",
    "\n",
    "test_strs = ['caresses', 'flies', 'dies', 'mules', 'denied',\n",
    "         'died', 'agreed', 'owned', 'humbled', 'sized',\n",
    "         'meeting', 'stating', 'siezing', 'itemization',\n",
    "         'sensational', 'traditional', 'reference', 'colonizer',\n",
    "         'plotted']\n",
    "\n",
    "singles = [stemmer.stem(word) for word in test_strs]\n",
    "print(' '.join(singles))  # doctest: +NORMALIZE_WHITESPACE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 词袋向量： 把文本转换成向量 。 只有向量才能作为模型的输入。 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 方法1： 词袋模型（按照词语出现的个数）\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "vectorizer = CountVectorizer()\n",
    "corpus = [\n",
    "     'He is going from Beijing to Shanghai.',\n",
    "     'He denied my request, but he actually lied.',\n",
    "     'Mike lost the phone, and phone was in the car.',\n",
    "]\n",
    "X = vectorizer.fit_transform(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0 0 1 0 0 0 1 1 1 0 1 0 0 0 0 0 0 1 0 1 0]\n",
      " [1 0 0 1 0 1 0 0 2 0 0 1 0 0 1 0 1 0 0 0 0]\n",
      " [0 1 0 0 1 0 0 0 0 1 0 0 1 1 0 2 0 0 2 0 1]]\n",
      "['actually', 'and', 'beijing', 'but', 'car', 'denied', 'from', 'going', 'he', 'in', 'is', 'lied', 'lost', 'mike', 'my', 'phone', 'request', 'shanghai', 'the', 'to', 'was']\n"
     ]
    }
   ],
   "source": [
    "print (X.toarray())\n",
    "print (vectorizer.get_feature_names())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 方法2：词袋模型（tf-idf方法）\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "vectorizer = TfidfVectorizer(smooth_idf=False)\n",
    "X = vectorizer.fit_transform(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.         0.         0.39379499 0.         0.         0.\n",
      "  0.39379499 0.39379499 0.26372909 0.         0.39379499 0.\n",
      "  0.         0.         0.         0.         0.         0.39379499\n",
      "  0.         0.39379499 0.        ]\n",
      " [0.35819397 0.         0.         0.35819397 0.         0.35819397\n",
      "  0.         0.         0.47977335 0.         0.         0.35819397\n",
      "  0.         0.         0.35819397 0.         0.35819397 0.\n",
      "  0.         0.         0.        ]\n",
      " [0.         0.26726124 0.         0.         0.26726124 0.\n",
      "  0.         0.         0.         0.26726124 0.         0.\n",
      "  0.26726124 0.26726124 0.         0.53452248 0.         0.\n",
      "  0.53452248 0.         0.26726124]]\n",
      "['actually', 'and', 'beijing', 'but', 'car', 'denied', 'from', 'going', 'he', 'in', 'is', 'lied', 'lost', 'mike', 'my', 'phone', 'request', 'shanghai', 'the', 'to', 'was']\n"
     ]
    }
   ],
   "source": [
    "print (X.toarray())\n",
    "print (vectorizer.get_feature_names())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
